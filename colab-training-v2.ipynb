{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "import transformers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup TPU or GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are running on CPU. switch to GPU for full training\n",
      "No TPU detected. Running on CPU\n",
      "Strategy: <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000017A90CFA6C8>\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"we are running on CPU. switch to GPU for full training\")\n",
    "\n",
    "try:\n",
    "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  tf.config.experimental_connect_to_cluster(resolver)\n",
    "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "except ValueError:\n",
    "    print(\"No TPU detected. Running on CPU\")\n",
    "    strategy = tf.distribute.get_strategy() \n",
    "\n",
    "print('Strategy:', strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv files in the us-patent-phrase-to-phrase-matching\n",
    "# directory and store them in a list\n",
    "path = 'us-patent-phrase-to-phrase-matching'\n",
    "files = os.listdir(path)\n",
    "# read the csv files\n",
    "df_train = pd.read_csv(path + '/' + 'train.csv')\n",
    "df_test = pd.read_csv(path + '/' + 'test.csv')\n",
    "df_sample = pd.read_csv(path + '/' + 'sample_submission.csv')\n",
    "parsed = {x: [] for x in ['code', 'title', 'section', 'class', 'subclass', 'group', 'main_group']}\n",
    "os.chdir(path)\n",
    "for letter in 'ABCDEFGHY':\n",
    "    file = f'cpc-section-{letter}_20220201.txt'\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            vals = line.strip().split('\\t')\n",
    "            if len(vals) == 2:\n",
    "                parsed['code'].append(vals[0])\n",
    "                parsed['title'].append(vals[1])\n",
    "            elif len(vals) == 3:\n",
    "                parsed['code'].append(vals[0])\n",
    "                parsed['title'].append(vals[2])\n",
    "for i in range(len(parsed['code'])):\n",
    "    code = parsed['code'][i]\n",
    "    main_group = code.split('/')[-1] if \"/\" in code else None\n",
    "    group = code.split('/')[0][4:] if len(code) >= 5 else None\n",
    "    subclass = code[3] if len(code) >= 4 else None\n",
    "    class_ = code[1:3] if len(code) >= 3 else None\n",
    "    section = code[0] if len(code) >= 1 else None\n",
    "    \n",
    "    parsed['main_group'].append(main_group)\n",
    "    parsed['group'].append(group)\n",
    "    parsed['subclass'].append(subclass)\n",
    "    parsed['class'].append(class_)\n",
    "    parsed['section'].append(section)\n",
    "\n",
    "\n",
    "# merge both dataframes\n",
    "df_codes = pd.DataFrame.from_dict(parsed)\n",
    "codes = df_codes.rename(columns = {\"code\" : \"context\"})\n",
    "train_data=pd.merge(df_train,codes[[\"context\",\"title\"]],on=\"context\",how=\"left\")\n",
    "test_data=pd.merge(df_test,codes[[\"context\",\"title\"]],on=\"context\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - we will enable it after our intial training\n",
    "tf.config.optimizer.set_jit(True) \n",
    "class Config():\n",
    "    seed = 42\n",
    "    epochs = 10\n",
    "    num_folds = 5\n",
    "    max_length = 96 # 192\n",
    "    batch_size = 16 #64\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    base_model = \"anferico/bert-for-patents\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(dataset, split_val):\n",
    "    lengths = int(len(dataset) * split_val)\n",
    "    train_data = dataset[:lengths]\n",
    "    valid_data = dataset[lengths:]\n",
    "    return train_data, valid_data\n",
    "\n",
    "\n",
    "def dataset_load(train_data, test_data):\n",
    "    train_data['sep_token'] = '[SEP]'\n",
    "    train_data['cls_token'] = '[CLS]'\n",
    "    train_data['context_token'] = '[' + train_data.context + ']'\n",
    "    context_tokens = list(train_data.context_token.unique())\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    train_data, valid_data = dataset_split(dataset=train_data, split_val=0.9)\n",
    "    test_data['sep_token'] = '[SEP]'\n",
    "    test_data['cls_token'] = '[CLS]'\n",
    "    test_data['context_token'] = '[' + test_data.context + ']'\n",
    "    return train_data, valid_data, test_data, context_tokens\n",
    "\n",
    "# create a learning rate scheduler\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(\n",
    "                epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler\n",
    "\n",
    "\n",
    "def encode_text(text,\n",
    "                tokenizer,\n",
    "                max_length):\n",
    "\n",
    "    # With tokenizer's batch_encode_plus batch of both the sentences are\n",
    "    # encoded together and separated by [SEP] token.\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_tensors=\"tf\",\n",
    "    )\n",
    "\n",
    "    # Convert batch of encoded features to numpy array.\n",
    "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_masks\": attention_masks,\n",
    "        \"token_type_ids\": token_type_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29542 3283 36\n",
      "[0.0, 0.25, 0.5, 0.75, 1.0]\n",
      "['[C09]', '[B63]', '[A47]', '[C03]', '[F04]', '[B23]', '[G01]', '[B60]', '[B05]', '[C07]', '[B41]', '[C04]', '[D04]', '[B81]', '[H04]', '[H01]', '[C11]', '[C25]', '[G02]', '[C10]', '[H02]', '[B22]', '[G06]', '[B62]', '[H05]', '[A46]', '[A01]', '[F01]', '[F02]', '[C13]', '[G07]', '[E21]', '[B29]', '[B01]', '[F41]', '[B21]', '[A21]', '[D05]', '[C02]', '[B44]', '[A61]', '[C12]', '[G03]', '[D03]', '[C06]', '[F16]', '[F24]', '[F15]', '[G21]', '[E02]', '[B61]', '[E01]', '[C22]', '[F25]', '[H03]', '[D06]', '[D01]', '[C08]', '[A63]', '[B03]', '[A41]', '[B65]', '[B02]', '[E06]', '[B66]', '[G10]', '[A24]', '[F23]', '[G04]', '[D21]', '[F42]', '[E04]', '[C01]', '[B08]', '[E03]', '[B27]', '[A23]', '[G11]', '[A44]', '[A22]', '[A43]', '[F21]', '[F17]', '[B25]', '[G08]', '[B28]', '[B32]', '[C23]', '[B07]', '[F28]', '[B24]', '[F27]', '[C14]', '[B31]', '[E05]', '[F22]', '[F03]', '[B67]', '[B64]', '[G09]', '[G16]', '[G05]', '[A45]', '[C21]', '[A62]', '[F26]']\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data, context_tokens = dataset_load(train_data, test_data)\n",
    "labels = list(set(train_data[\"score\"].values))\n",
    "labels.sort()\n",
    "\n",
    "print(len(train_data), len(valid_data), len(test_data))\n",
    "print(labels)\n",
    "print(context_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['title'] = train_data['title'].str.lower()\n",
    "train_data['anchor'] = train_data['anchor'].str.lower()\n",
    "train_data['target'] = train_data['target'].str.lower()\n",
    "# Tokenizer.\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(Config.base_model)\n",
    "# Context tokens. \n",
    "train_data['context_token'] = '[' + train_data.context + ']'\n",
    "train_data['sep_token'] = '[SEP]'\n",
    "train_data['cls_token'] = '[CLS]'\n",
    "context_tokens = list(train_data.context_token.unique())\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': context_tokens})\n",
    "\n",
    "# Preparing input text for the model.\n",
    "# We are adding context_token before the context title\n",
    "# to let model learn the context of anchor and target.\n",
    "train_data['text'] = train_data['cls_token'] + \\\n",
    "                    train_data['context_token'] + train_data['title'] + \\\n",
    "                    train_data['sep_token'] + train_data['anchor'] + \\\n",
    "                    train_data['sep_token'] + train_data['target'] + \\\n",
    "                train_data['sep_token']\n",
    "\n",
    "test_data['title'] = test_data['title'].str.lower().str.replace(\";\",\"\")\n",
    "test_data['anchor'] = test_data['anchor'].str.lower()\n",
    "test_data['target'] = test_data['target'].str.lower()\n",
    "\n",
    "test_data['text'] = test_data['title'] + \" \" + test_data['anchor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>sep_token</th>\n",
       "      <th>cls_token</th>\n",
       "      <th>context_token</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e2e6e9aa50cd8a1</td>\n",
       "      <td>define by memory</td>\n",
       "      <td>store by memory</td>\n",
       "      <td>G06</td>\n",
       "      <td>0.50</td>\n",
       "      <td>computing; calculating; counting</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[G06]</td>\n",
       "      <td>[CLS][G06]computing; calculating; counting[SEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1434a6f3ecb7d5b3</td>\n",
       "      <td>linear systems</td>\n",
       "      <td>rotating tool</td>\n",
       "      <td>B23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>machine tools; metal-working not otherwise pro...</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[B23]</td>\n",
       "      <td>[CLS][B23]machine tools; metal-working not oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b709cd34cd3e5c43</td>\n",
       "      <td>metatarsal bones</td>\n",
       "      <td>forefoot bones</td>\n",
       "      <td>A61</td>\n",
       "      <td>0.75</td>\n",
       "      <td>medical or veterinary science; hygiene</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[A61]</td>\n",
       "      <td>[CLS][A61]medical or veterinary science; hygie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e99070faf001ce93</td>\n",
       "      <td>generated electrical power</td>\n",
       "      <td>alternating signal</td>\n",
       "      <td>H02</td>\n",
       "      <td>0.50</td>\n",
       "      <td>generation; conversion or distribution of elec...</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[H02]</td>\n",
       "      <td>[CLS][H02]generation; conversion or distributi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f0b4938c61b5b89</td>\n",
       "      <td>high gradient magnetic separators</td>\n",
       "      <td>magnetic separator</td>\n",
       "      <td>C02</td>\n",
       "      <td>0.50</td>\n",
       "      <td>treatment of water, waste water, sewage, or sl...</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[C02]</td>\n",
       "      <td>[CLS][C02]treatment of water, waste water, sew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                             anchor              target  \\\n",
       "0  9e2e6e9aa50cd8a1                   define by memory     store by memory   \n",
       "1  1434a6f3ecb7d5b3                     linear systems       rotating tool   \n",
       "2  b709cd34cd3e5c43                   metatarsal bones      forefoot bones   \n",
       "3  e99070faf001ce93         generated electrical power  alternating signal   \n",
       "4  9f0b4938c61b5b89  high gradient magnetic separators  magnetic separator   \n",
       "\n",
       "  context  score                                              title sep_token  \\\n",
       "0     G06   0.50                   computing; calculating; counting     [SEP]   \n",
       "1     B23   0.25  machine tools; metal-working not otherwise pro...     [SEP]   \n",
       "2     A61   0.75             medical or veterinary science; hygiene     [SEP]   \n",
       "3     H02   0.50  generation; conversion or distribution of elec...     [SEP]   \n",
       "4     C02   0.50  treatment of water, waste water, sewage, or sl...     [SEP]   \n",
       "\n",
       "  cls_token context_token                                               text  \n",
       "0     [CLS]         [G06]  [CLS][G06]computing; calculating; counting[SEP...  \n",
       "1     [CLS]         [B23]  [CLS][B23]machine tools; metal-working not oth...  \n",
       "2     [CLS]         [A61]  [CLS][A61]medical or veterinary science; hygie...  \n",
       "3     [CLS]         [H02]  [CLS][H02]generation; conversion or distributi...  \n",
       "4     [CLS]         [C02]  [CLS][C02]treatment of water, waste water, sew...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>title</th>\n",
       "      <th>sep_token</th>\n",
       "      <th>cls_token</th>\n",
       "      <th>context_token</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>opc drum</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "      <td>optics</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[G02]</td>\n",
       "      <td>optics opc drum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>adjust gas flow</td>\n",
       "      <td>altering gas flow</td>\n",
       "      <td>F23</td>\n",
       "      <td>combustion apparatus combustion processes</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[F23]</td>\n",
       "      <td>combustion apparatus combustion processes adju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>lower trunnion</td>\n",
       "      <td>lower locating</td>\n",
       "      <td>B60</td>\n",
       "      <td>vehicles in general</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[B60]</td>\n",
       "      <td>vehicles in general lower trunnion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>cap component</td>\n",
       "      <td>upper portion</td>\n",
       "      <td>D06</td>\n",
       "      <td>treatment of textiles or the like laundering f...</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[D06]</td>\n",
       "      <td>treatment of textiles or the like laundering f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>neural stimulation</td>\n",
       "      <td>artificial neural network</td>\n",
       "      <td>H04</td>\n",
       "      <td>electric communication technique</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[H04]</td>\n",
       "      <td>electric communication technique neural stimul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              anchor                         target  \\\n",
       "0  4112d61851461f60            opc drum  inorganic photoconductor drum   \n",
       "1  09e418c93a776564     adjust gas flow              altering gas flow   \n",
       "2  36baf228038e314b      lower trunnion                 lower locating   \n",
       "3  1f37ead645e7f0c8       cap component                  upper portion   \n",
       "4  71a5b6ad068d531f  neural stimulation      artificial neural network   \n",
       "\n",
       "  context                                              title sep_token  \\\n",
       "0     G02                                             optics     [SEP]   \n",
       "1     F23          combustion apparatus combustion processes     [SEP]   \n",
       "2     B60                                vehicles in general     [SEP]   \n",
       "3     D06  treatment of textiles or the like laundering f...     [SEP]   \n",
       "4     H04                   electric communication technique     [SEP]   \n",
       "\n",
       "  cls_token context_token                                               text  \n",
       "0     [CLS]         [G02]                                    optics opc drum  \n",
       "1     [CLS]         [F23]  combustion apparatus combustion processes adju...  \n",
       "2     [CLS]         [B60]                 vehicles in general lower trunnion  \n",
       "3     [CLS]         [D06]  treatment of textiles or the like laundering f...  \n",
       "4     [CLS]         [H04]  electric communication technique neural stimul...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_data = encode_text(test_data[[\"text\", \"target\"]].values.tolist(), tokenizer, Config.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2 20691  6393  1943  6608     3 27921  5967  8328  8231 16426  6608\n",
      "     3     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_test_data[\"input_ids\"][0])\n",
    "print(encoded_test_data[\"attention_masks\"][0])\n",
    "print(encoded_test_data[\"token_type_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test x shape :  (36, 96) (36, 96) (36, 96)\n"
     ]
    }
   ],
   "source": [
    "test_x = [encoded_test_data[\"input_ids\"], encoded_test_data[\"attention_masks\"], encoded_test_data[\"token_type_ids\"]]\n",
    "print(\"test x shape : \", test_x[0].shape, test_x[1].shape, test_x[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config):\n",
    "    # Create the model under a distribution strategy scope.\n",
    "    #with strategy.scope():\n",
    "        # Encoded token ids from BERT tokenizer.\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(config.max_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(config.max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(config.max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "    # Loading pretrained BERT model.\n",
    "    base_model = transformers.TFAutoModel.from_pretrained(config.base_model, from_pt=True)\n",
    "\n",
    "    base_model_output = base_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "\n",
    "    last_hidden_state = base_model_output.last_hidden_state\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(last_hidden_state)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(avg_pool)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'bert.embeddings.position_ids', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 344702976   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 1024)         0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 1024)         0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        dropout_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 344,704,001\n",
      "Trainable params: 344,704,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model =build_model(Config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_weights('model-2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets predict the test data.\n",
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read sample submission file.\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['score'] = predictions\n",
    "submission['score'] = submission.score.apply(lambda x: 0 if x < 0 else x)\n",
    "submission['score'] = submission.score.apply(lambda x: 1 if x > 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.363816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.363814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.363814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.363816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b9fdc772bb8fd61c</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7aa5908a77a7ec24</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>d19ef3979396d47e</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fd83613b7843f5e1</td>\n",
       "      <td>0.363814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2a619016908bfa45</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>733979d75f59770d</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6546846df17f9800</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3ff0e7a35015be69</td>\n",
       "      <td>0.363814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12ca31f018a2e2b9</td>\n",
       "      <td>0.363816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>03ba802ed4029e4d</td>\n",
       "      <td>0.363814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>c404f8b378cbb008</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>78243984c02a72e4</td>\n",
       "      <td>0.363813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>de51114bc0faec3e</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7e3aff857f056bf9</td>\n",
       "      <td>0.363814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>26c3c6dc6174b589</td>\n",
       "      <td>0.363814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b892011ab2e2cabc</td>\n",
       "      <td>0.363816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8247ff562ca185cc</td>\n",
       "      <td>0.363814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>c057aecbba832387</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9f2279ce667b21dc</td>\n",
       "      <td>0.363816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>b9ea2b06a878df6f</td>\n",
       "      <td>0.363816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>79795133c30ef097</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25522ee5411e63e9</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.363815\n",
       "1   09e418c93a776564  0.363816\n",
       "2   36baf228038e314b  0.363814\n",
       "3   1f37ead645e7f0c8  0.363814\n",
       "4   71a5b6ad068d531f  0.363816\n",
       "5   474c874d0c07bd21  0.363815\n",
       "6   442c114ed5c4e3c9  0.363815\n",
       "7   b8ae62ea5e1d8bdb  0.363815\n",
       "8   faaddaf8fcba8a3f  0.363815\n",
       "9   ae0262c02566d2ce  0.363815\n",
       "10  a8808e31641e856d  0.363815\n",
       "11  16ae4b99d3601e60  0.363815\n",
       "12  25c555ca3d5a2092  0.363815\n",
       "13  5203a36c501f1b7c  0.363815\n",
       "14  b9fdc772bb8fd61c  0.363815\n",
       "15  7aa5908a77a7ec24  0.363815\n",
       "16  d19ef3979396d47e  0.363815\n",
       "17  fd83613b7843f5e1  0.363814\n",
       "18  2a619016908bfa45  0.363815\n",
       "19  733979d75f59770d  0.363815\n",
       "20  6546846df17f9800  0.363815\n",
       "21  3ff0e7a35015be69  0.363814\n",
       "22  12ca31f018a2e2b9  0.363816\n",
       "23  03ba802ed4029e4d  0.363814\n",
       "24  c404f8b378cbb008  0.363815\n",
       "25  78243984c02a72e4  0.363813\n",
       "26  de51114bc0faec3e  0.363815\n",
       "27  7e3aff857f056bf9  0.363814\n",
       "28  26c3c6dc6174b589  0.363814\n",
       "29  b892011ab2e2cabc  0.363816\n",
       "30  8247ff562ca185cc  0.363814\n",
       "31  c057aecbba832387  0.363815\n",
       "32  9f2279ce667b21dc  0.363816\n",
       "33  b9ea2b06a878df6f  0.363816\n",
       "34  79795133c30ef097  0.363815\n",
       "35  25522ee5411e63e9  0.363815"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24ffb122db4d293cbce3a5b97151e6069f3c74d2302b3d157e23222a76757a60"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kaggle-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
